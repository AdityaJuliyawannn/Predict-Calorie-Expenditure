{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "03ca0216-f6fa-43d5-95aa-00639608d287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.6.0\n"
     ]
    }
   ],
   "source": [
    "import lightgbm\n",
    "print(lightgbm.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "77e74e53-861c-460c-a17c-438b6376509a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder, PolynomialFeatures, StandardScaler\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import itertools\n",
    "from scipy.optimize import minimize\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0626e6d8-74fb-464b-b73e-9a0cbfdc7ea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train shape: (750000, 9)\n",
      "Test shape: (250000, 8)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(r\"C:\\Users\\Aditya P J\\Documents\\Competition\\Kaggle\\train.csv\")\n",
    "test = pd.read_csv(r\"C:\\Users\\Aditya P J\\Documents\\Competition\\Kaggle\\test.csv\")\n",
    "submission = pd.read_csv(r\"C:\\Users\\Aditya P J\\Documents\\Competition\\Kaggle\\sample_submission.csv\")\n",
    "\n",
    "# Define features\n",
    "numerical_features = ['Age', 'Height', 'Weight', 'Duration', 'Heart_Rate', 'Body_Temp']\n",
    "categorical_features = ['Sex']\n",
    "\n",
    "# Print data information\n",
    "print(\"Train shape:\", train.shape)\n",
    "print(\"Test shape:\", test.shape)\n",
    "\n",
    "# Label encode categorical features\n",
    "le = LabelEncoder()\n",
    "train['Sex'] = le.fit_transform(train['Sex'])\n",
    "test['Sex'] = le.transform(test['Sex'])\n",
    "\n",
    "# Transform target\n",
    "train['Calories_log'] = np.log1p(train['Calories'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "41fb532f-05a3-4197-8600-8b8988753511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape: (750000, 75)\n",
      "X_test shape: (250000, 75)\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering functions\n",
    "def create_ratio_features(df):\n",
    "    df = df.copy()\n",
    "    # BMI and related metrics\n",
    "    df['BMI'] = df['Weight'] / ((df['Height']/100) ** 2)\n",
    "    df['BMI_Prime'] = df['BMI'] / 25.0  # Normalized BMI\n",
    "    \n",
    "    # Exercise intensity metrics\n",
    "    df['Heart_Rate_per_kg'] = df['Heart_Rate'] / df['Weight']\n",
    "    df['Duration_per_HR'] = df['Duration'] / df['Heart_Rate']\n",
    "    \n",
    "    # Energy expenditure related\n",
    "    df['HR_Weight_Ratio'] = df['Heart_Rate'] / df['Weight']\n",
    "    df['Intensity_Factor'] = df['Heart_Rate'] * df['Duration'] / 60\n",
    "    df['Temp_HR_Ratio'] = df['Body_Temp'] / df['Heart_Rate']\n",
    "    \n",
    "    # Special exercise-specific formulas\n",
    "    df['Energy_Index'] = df['Heart_Rate'] * df['Duration'] * df['Weight'] / 10000\n",
    "    df['Calorie_Estimator'] = ((0.2 * df['Heart_Rate']) + (0.1 * df['Weight']) + (0.05 * df['Duration'])) * 5\n",
    "    \n",
    "    # NEW: Karvonen formula related features\n",
    "    df['Max_HR'] = 220 - df['Age']\n",
    "    df['HR_Reserve'] = df['Max_HR'] - 60  # Assuming resting HR of 60\n",
    "    df['HR_Reserve_Used'] = (df['Heart_Rate'] - 60) / df['HR_Reserve']\n",
    "    \n",
    "    # NEW: Age adjusted features\n",
    "    df['Age_Adjusted_HR'] = df['Heart_Rate'] / (220 - df['Age'])\n",
    "    df['Age_Weight_Interaction'] = df['Age'] * df['Weight'] / 100\n",
    "    \n",
    "    # NEW: Metabolic features\n",
    "    df['MET_estimate'] = 3.5 + (df['Heart_Rate'] - 60) * 0.1\n",
    "    df['Est_VO2'] = (df['Heart_Rate'] / df['Max_HR']) * 100\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def add_interaction_features(df, features):\n",
    "    df_new = df.copy()\n",
    "    # Create all pairwise feature interactions (limited to most meaningful ones)\n",
    "    important_pairs = [\n",
    "        ('Duration', 'Heart_Rate'),\n",
    "        ('Weight', 'Heart_Rate'),\n",
    "        ('Duration', 'Weight'),\n",
    "        ('Age', 'Heart_Rate'),\n",
    "        ('Height', 'Weight'),\n",
    "        ('Body_Temp', 'Heart_Rate'),\n",
    "        ('Age', 'Weight'),\n",
    "        ('Duration', 'Body_Temp')\n",
    "    ]\n",
    "    \n",
    "    for f1, f2 in important_pairs:\n",
    "        # Multiplication (most important)\n",
    "        df_new[f\"{f1}_x_{f2}\"] = df_new[f1] * df_new[f2]\n",
    "        \n",
    "        # Addition\n",
    "        df_new[f\"{f1}_plus_{f2}\"] = df_new[f1] + df_new[f2]\n",
    "        \n",
    "        # Division (both ways)\n",
    "        df_new[f\"{f1}_div_{f2}\"] = df_new[f1] / (df_new[f2] + 1e-5)\n",
    "        df_new[f\"{f2}_div_{f1}\"] = df_new[f2] / (df_new[f1] + 1e-5)\n",
    "        \n",
    "        # NEW: Square root of product\n",
    "        df_new[f\"sqrt_{f1}_x_{f2}\"] = np.sqrt(df_new[f1] * df_new[f2] + 1e-5)\n",
    "        \n",
    "        # NEW: Log of product\n",
    "        df_new[f\"log_{f1}_x_{f2}\"] = np.log1p(df_new[f1] * df_new[f2])\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "def add_statistical_features(df, features):\n",
    "    df_new = df.copy()\n",
    "    df_new[\"row_mean\"] = df[features].mean(axis=1)\n",
    "    df_new[\"row_std\"] = df[features].std(axis=1)\n",
    "    df_new[\"row_max\"] = df[features].max(axis=1)\n",
    "    df_new[\"row_min\"] = df[features].min(axis=1)\n",
    "    df_new[\"row_range\"] = df_new[\"row_max\"] - df_new[\"row_min\"]\n",
    "    \n",
    "    # Calculate mean absolute deviation manually\n",
    "    df_new[\"row_mad\"] = df[features].sub(df[features].mean(axis=1), axis=0).abs().mean(axis=1)\n",
    "    \n",
    "    # NEW: Coefficient of variation\n",
    "    df_new[\"row_cv\"] = df_new[\"row_std\"] / (df_new[\"row_mean\"] + 1e-5)\n",
    "    \n",
    "    # NEW: Z-score for each feature\n",
    "    for feat in features:\n",
    "        df_new[f\"{feat}_zscore\"] = (df[feat] - df[feat].mean()) / (df[feat].std() + 1e-5)\n",
    "    \n",
    "    return df_new\n",
    "\n",
    "def add_exercise_features(df):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Exercise science formulas\n",
    "    df['MET_estimate'] = 3.5 + (df['Heart_Rate'] - 60) * 0.1\n",
    "    df['HR_reserve_pct'] = (df['Heart_Rate'] - 60) / ((220 - df['Age']) - 60)\n",
    "    \n",
    "    # Advanced intensity metrics\n",
    "    df['sqrt_Duration'] = np.sqrt(df['Duration'])\n",
    "    df['log_Weight'] = np.log1p(df['Weight'])\n",
    "    df['log_Duration'] = np.log1p(df['Duration'])\n",
    "    \n",
    "    # Non-linear Heart Rate transformations\n",
    "    df['HR_squared'] = df['Heart_Rate'] ** 2\n",
    "    df['HR_cubed'] = df['Heart_Rate'] ** 3\n",
    "    df['HR_sqrt'] = np.sqrt(df['Heart_Rate'])\n",
    "    \n",
    "    # Cardiovascular load metrics\n",
    "    df['CV_Load'] = df['Heart_Rate'] * df['Duration'] / 1000\n",
    "    df['CV_Load_Weight_Adjusted'] = df['CV_Load'] / df['Weight']\n",
    "    \n",
    "    # NEW: Power features\n",
    "    df['Power_estimate'] = df['Weight'] * df['Heart_Rate'] * df['Duration'] / 10000\n",
    "    \n",
    "    # NEW: Exponential transformations\n",
    "    df['exp_HR_scaled'] = np.exp(df['Heart_Rate']/100) - 1\n",
    "    df['exp_Duration_scaled'] = np.exp(df['Duration']/100) - 1\n",
    "    \n",
    "    # NEW: Fatigue features\n",
    "    df['Fatigue_Index'] = df['Duration'] * (df['Heart_Rate'] / (220 - df['Age']))\n",
    "    \n",
    "    # NEW: Intensity zones (based on HR zones)\n",
    "    df['HR_pct_max'] = df['Heart_Rate'] / (220 - df['Age'])\n",
    "    \n",
    "    # NEW: Physiological cost index\n",
    "    df['PCI'] = df['Heart_Rate'] / df['Duration']\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def add_target_encoding(X_train, X_test, y, cat_cols=['Sex']):\n",
    "    X_train_enc = X_train.copy()\n",
    "    X_test_enc = X_test.copy()\n",
    "    \n",
    "    # Use stratified KFold for more robust encoding\n",
    "    kf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    \n",
    "    # Process each categorical column\n",
    "    for col in cat_cols:\n",
    "        if col in X_train.columns:\n",
    "            # Initialize temporary array for out-of-fold predictions\n",
    "            X_train_enc[f'{col}_target_mean'] = np.zeros(len(X_train))\n",
    "            \n",
    "            # Global target mean (for handling unseen categories)\n",
    "            global_mean = y.mean()\n",
    "            \n",
    "            # Dictionary to store encodings for test data\n",
    "            encoding_dict = {}\n",
    "            \n",
    "            # Cross-validation loop\n",
    "            for train_idx, val_idx in kf.split(X_train):\n",
    "                # Get target means from training fold\n",
    "                target_means = y.iloc[train_idx].groupby(X_train[col].iloc[train_idx]).mean()\n",
    "                \n",
    "                # Apply to validation fold\n",
    "                for category, mean_value in target_means.items():\n",
    "                    val_indices = val_idx[X_train[col].iloc[val_idx] == category]\n",
    "                    X_train_enc.loc[val_indices, f'{col}_target_mean'] = mean_value\n",
    "                    \n",
    "                    # Update dictionary for test set\n",
    "                    if category in encoding_dict:\n",
    "                        encoding_dict[category] = (encoding_dict[category] + mean_value) / 2\n",
    "                    else:\n",
    "                        encoding_dict[category] = mean_value\n",
    "            \n",
    "            # Apply encodings to test data\n",
    "            X_test_enc[f'{col}_target_mean'] = X_test[col].map(encoding_dict).fillna(global_mean)\n",
    "            \n",
    "            # NEW: Add variance of target for each category\n",
    "            target_vars = y.groupby(X_train[col]).var().to_dict()\n",
    "            X_train_enc[f'{col}_target_var'] = X_train[col].map(target_vars).fillna(y.var())\n",
    "            X_test_enc[f'{col}_target_var'] = X_test[col].map(target_vars).fillna(y.var())\n",
    "    \n",
    "    return X_train_enc, X_test_enc\n",
    "\n",
    "def drop_highly_correlated(df, threshold=0.995):\n",
    "    # Find and drop highly correlated features\n",
    "    corr_matrix = df.corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [col for col in upper.columns if any(upper[col] > threshold)]\n",
    "    return df.drop(columns=to_drop), to_drop\n",
    "\n",
    "# Create polynomial features\n",
    "def add_polynomial_features(X_train, X_test, features, degree=2):\n",
    "    poly = PolynomialFeatures(degree=degree, interaction_only=True, include_bias=False)\n",
    "    \n",
    "    # Fit on training data\n",
    "    poly_features_train = poly.fit_transform(X_train[features])\n",
    "    feature_names = poly.get_feature_names_out(features)\n",
    "    \n",
    "    # Create DataFrame with new features\n",
    "    poly_train = pd.DataFrame(poly_features_train, columns=feature_names)\n",
    "    \n",
    "    # Apply to test data\n",
    "    poly_features_test = poly.transform(X_test[features])\n",
    "    poly_test = pd.DataFrame(poly_features_test, columns=feature_names)\n",
    "    \n",
    "    # Remove original features to avoid duplication\n",
    "    poly_train = poly_train[[col for col in poly_train.columns if col not in features]]\n",
    "    poly_test = poly_test[[col for col in poly_test.columns if col not in features]]\n",
    "    \n",
    "    # Concatenate with original dataframes\n",
    "    X_train_poly = pd.concat([X_train.reset_index(drop=True), poly_train.reset_index(drop=True)], axis=1)\n",
    "    X_test_poly = pd.concat([X_test.reset_index(drop=True), poly_test.reset_index(drop=True)], axis=1)\n",
    "    \n",
    "    return X_train_poly, X_test_poly\n",
    "\n",
    "def add_cluster_features(X_train, X_test, n_clusters=5):\n",
    "    from sklearn.cluster import KMeans\n",
    "    \n",
    "    # Select subset of features for clustering\n",
    "    cluster_features = ['Age', 'Weight', 'Heart_Rate', 'Duration', 'Body_Temp']\n",
    "    available_features = [f for f in cluster_features if f in X_train.columns]\n",
    "    \n",
    "    if len(available_features) < 3:\n",
    "        return X_train, X_test\n",
    "        \n",
    "    # Standardize data for clustering\n",
    "    scaler = StandardScaler()\n",
    "    train_scaled = scaler.fit_transform(X_train[available_features])\n",
    "    test_scaled = scaler.transform(X_test[available_features])\n",
    "    \n",
    "    # Fit KMeans\n",
    "    kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "    train_clusters = kmeans.fit_predict(train_scaled)\n",
    "    test_clusters = kmeans.predict(test_scaled)\n",
    "    \n",
    "    # Add cluster labels as features\n",
    "    X_train_new = X_train.copy()\n",
    "    X_test_new = X_test.copy()\n",
    "    \n",
    "    X_train_new['cluster'] = train_clusters\n",
    "    X_test_new['cluster'] = test_clusters\n",
    "    \n",
    "    # Add distances to centroids\n",
    "    distances = kmeans.transform(train_scaled)\n",
    "    for i in range(n_clusters):\n",
    "        X_train_new[f'cluster_dist_{i}'] = distances[:, i]\n",
    "    \n",
    "    distances = kmeans.transform(test_scaled)\n",
    "    for i in range(n_clusters):\n",
    "        X_test_new[f'cluster_dist_{i}'] = distances[:, i]\n",
    "    \n",
    "    # Create dummy variables for clusters\n",
    "    train_dummies = pd.get_dummies(train_clusters, prefix='cluster')\n",
    "    test_dummies = pd.get_dummies(test_clusters, prefix='cluster')\n",
    "    \n",
    "    # Ensure all clusters are represented in test set\n",
    "    for i in range(n_clusters):\n",
    "        if f'cluster_{i}' not in test_dummies.columns:\n",
    "            test_dummies[f'cluster_{i}'] = 0\n",
    "    \n",
    "    X_train_new = pd.concat([X_train_new, train_dummies], axis=1)\n",
    "    X_test_new = pd.concat([X_test_new, test_dummies[train_dummies.columns]], axis=1)\n",
    "    \n",
    "    return X_train_new, X_test_new\n",
    "    \n",
    "# Feature creation pipeline\n",
    "def create_features(train_df, test_df):\n",
    "    # Copy data\n",
    "    train_df = train_df.copy()\n",
    "    test_df = test_df.copy()\n",
    "    \n",
    "    # 1. Add basic ratio features\n",
    "    train_df = create_ratio_features(train_df)\n",
    "    test_df = create_ratio_features(test_df)\n",
    "    \n",
    "    # 2. Add exercise-specific features\n",
    "    train_df = add_exercise_features(train_df)\n",
    "    test_df = add_exercise_features(test_df)\n",
    "    \n",
    "    # 3. Add interaction features\n",
    "    train_df = add_interaction_features(train_df, numerical_features)\n",
    "    test_df = add_interaction_features(test_df, numerical_features)\n",
    "    \n",
    "    # 4. Add statistical features\n",
    "    train_df = add_statistical_features(train_df, numerical_features)\n",
    "    test_df = add_statistical_features(test_df, numerical_features)\n",
    "    \n",
    "    # 5. Create polynomial features \n",
    "    train_poly, test_poly = add_polynomial_features(\n",
    "        train_df.drop(columns=['id', 'Calories', 'Calories_log'] if 'Calories_log' in train_df.columns else ['id', 'Calories']), \n",
    "        test_df.drop(columns=['id']), \n",
    "        numerical_features, \n",
    "        degree=2\n",
    "    )\n",
    "    \n",
    "    # 6. Add cluster features\n",
    "    train_poly, test_poly = add_cluster_features(train_poly, test_poly, n_clusters=8)\n",
    "    \n",
    "    # 7. Prepare X and y\n",
    "    X = train_poly\n",
    "    y = train_df['Calories_log'] if 'Calories_log' in train_df.columns else np.log1p(train_df['Calories'])\n",
    "    X_test = test_poly\n",
    "    \n",
    "    # 8. Add target encoding\n",
    "    X, X_test = add_target_encoding(X, X_test, y, cat_cols=['Sex'])\n",
    "    \n",
    "    # 9. Drop highly correlated features\n",
    "    X, dropped_cols = drop_highly_correlated(X, threshold=0.995)\n",
    "    X_test = X_test.drop(columns=[col for col in dropped_cols if col in X_test.columns])\n",
    "    \n",
    "    # 10. Scale numerical features\n",
    "    scaler = StandardScaler()\n",
    "    numerical_cols = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    \n",
    "    X[numerical_cols] = scaler.fit_transform(X[numerical_cols])\n",
    "    X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])\n",
    "    \n",
    "    return X, X_test, y\n",
    "\n",
    "# Create features\n",
    "X, X_test, y = create_features(train, test)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")\n",
    "print(f\"X_test shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "aab088d9-e70a-4366-955f-79dbebf889d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training CatBoost ===\n",
      "\n",
      "Fold 1\n",
      "0:\tlearn: 0.9538152\ttest: 0.9547609\tbest: 0.9547609 (0)\ttotal: 461ms\tremaining: 19m 12s\n",
      "100:\tlearn: 0.3675573\ttest: 0.3682006\tbest: 0.3682006 (100)\ttotal: 45.1s\tremaining: 17m 51s\n",
      "200:\tlearn: 0.1539684\ttest: 0.1548560\tbest: 0.1548560 (200)\ttotal: 1m 30s\tremaining: 17m 10s\n",
      "300:\tlearn: 0.0837300\ttest: 0.0851460\tbest: 0.0851460 (300)\ttotal: 2m 18s\tremaining: 16m 50s\n",
      "400:\tlearn: 0.0657986\ttest: 0.0675918\tbest: 0.0675918 (400)\ttotal: 3m 6s\tremaining: 16m 15s\n",
      "500:\tlearn: 0.0616675\ttest: 0.0634993\tbest: 0.0634993 (500)\ttotal: 3m 50s\tremaining: 15m 18s\n",
      "600:\tlearn: 0.0603512\ttest: 0.0621743\tbest: 0.0621743 (600)\ttotal: 4m 35s\tremaining: 14m 30s\n",
      "700:\tlearn: 0.0597415\ttest: 0.0615706\tbest: 0.0615706 (700)\ttotal: 5m 20s\tremaining: 13m 42s\n",
      "800:\tlearn: 0.0593708\ttest: 0.0612529\tbest: 0.0612529 (800)\ttotal: 6m 4s\tremaining: 12m 52s\n",
      "900:\tlearn: 0.0590771\ttest: 0.0610475\tbest: 0.0610475 (900)\ttotal: 6m 47s\tremaining: 12m 2s\n",
      "1000:\tlearn: 0.0588388\ttest: 0.0608930\tbest: 0.0608930 (1000)\ttotal: 7m 29s\tremaining: 11m 12s\n",
      "1100:\tlearn: 0.0586312\ttest: 0.0607685\tbest: 0.0607685 (1100)\ttotal: 8m 10s\tremaining: 10m 23s\n",
      "1200:\tlearn: 0.0584333\ttest: 0.0606807\tbest: 0.0606807 (1200)\ttotal: 8m 52s\tremaining: 9m 35s\n",
      "1300:\tlearn: 0.0582496\ttest: 0.0605972\tbest: 0.0605972 (1300)\ttotal: 9m 33s\tremaining: 8m 48s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[81], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m train_pool \u001b[38;5;241m=\u001b[39m Pool(x_train, y_train, cat_features\u001b[38;5;241m=\u001b[39mcat_features)\n\u001b[0;32m     64\u001b[0m valid_pool \u001b[38;5;241m=\u001b[39m Pool(x_valid, y_valid, cat_features\u001b[38;5;241m=\u001b[39mcat_features)\n\u001b[1;32m---> 65\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_pool, eval_set\u001b[38;5;241m=\u001b[39mvalid_pool, early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Make predictions\u001b[39;00m\n\u001b[0;32m     68\u001b[0m oof_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(x_valid)\n",
      "File \u001b[1;32mD:\\Terminal\\Lib\\site-packages\\catboost\\core.py:5873\u001b[0m, in \u001b[0;36mCatBoostRegressor.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, graph, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5871\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5872\u001b[0m     CatBoostRegressor\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5873\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(X, y, cat_features, text_features, embedding_features, \u001b[38;5;28;01mNone\u001b[39;00m, graph, sample_weight, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, baseline,\n\u001b[0;32m   5874\u001b[0m                  use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description,\n\u001b[0;32m   5875\u001b[0m                  verbose_eval, metric_period, silent, early_stopping_rounds,\n\u001b[0;32m   5876\u001b[0m                  save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\n",
      "File \u001b[1;32mD:\\Terminal\\Lib\\site-packages\\catboost\\core.py:2410\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, graph, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2407\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2409\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2410\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train(\n\u001b[0;32m   2411\u001b[0m         train_pool,\n\u001b[0;32m   2412\u001b[0m         train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_sets\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m   2413\u001b[0m         params,\n\u001b[0;32m   2414\u001b[0m         allow_clear_pool,\n\u001b[0;32m   2415\u001b[0m         train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minit_model\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2416\u001b[0m     )\n\u001b[0;32m   2418\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2419\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32mD:\\Terminal\\Lib\\site-packages\\catboost\\core.py:1790\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1790\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_train(train_pool, test_pool, params, allow_clear_pool, init_model\u001b[38;5;241m.\u001b[39m_object \u001b[38;5;28;01mif\u001b[39;00m init_model \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m   1791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:5023\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:5072\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "\n",
    "# Load data - assumes X, y, X_test, and submission are already prepared\n",
    "# If running this as standalone script, uncomment and modify these lines:\n",
    "# X = pd.read_csv('train_features.csv')\n",
    "# y = pd.read_csv('train_target.csv').squeeze()\n",
    "# X_test = pd.read_csv('test_features.csv')\n",
    "# submission = pd.read_csv('sample_submission.csv')\n",
    "\n",
    "# Define model parameters\n",
    "catboost_params = {\n",
    "    'iterations': 2500,\n",
    "    'learning_rate': 0.01,\n",
    "    'depth': 10,\n",
    "    'l2_leaf_reg': 3.0,\n",
    "    'random_strength': 0.8,\n",
    "    'bagging_temperature': 1.0,\n",
    "    'od_type': 'Iter',\n",
    "    'od_wait': 100,\n",
    "    'boosting_type': 'Plain',  # Try 'Plain' instead of default\n",
    "    'grow_policy': 'SymmetricTree',  # Better for regression tasks\n",
    "    'verbose': 100,\n",
    "    'random_seed': 42\n",
    "}\n",
    "\n",
    "# Number of folds for cross-validation\n",
    "FOLDS = 10\n",
    "\n",
    "# Create bins for stratification\n",
    "bins = pd.qcut(y, 10, labels=False, duplicates='drop')\n",
    "\n",
    "# Use StratifiedKFold for better distribution\n",
    "kf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# Initialize results\n",
    "cat_oof = np.zeros(len(X))\n",
    "cat_pred = np.zeros(len(X_test))\n",
    "cat_rmsle = []\n",
    "\n",
    "print(\"\\n=== Training CatBoost ===\")\n",
    "model = CatBoostRegressor(**catboost_params)\n",
    "\n",
    "# Train on each fold\n",
    "for i, (train_idx, valid_idx) in enumerate(kf.split(X, bins)):\n",
    "    print(f\"\\nFold {i+1}\")\n",
    "    x_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    x_valid, y_valid = X.iloc[valid_idx], y.iloc[valid_idx]\n",
    "    \n",
    "    # Handle duplicate columns\n",
    "    x_train = x_train.loc[:, ~x_train.columns.duplicated()]\n",
    "    x_valid = x_valid.loc[:, ~x_valid.columns.duplicated()]\n",
    "    x_test = X_test.loc[:, ~X_test.columns.duplicated()].copy()\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    model = CatBoostRegressor(**catboost_params)\n",
    "    cat_features = ['Sex'] if 'Sex' in x_train.columns else None\n",
    "    train_pool = Pool(x_train, y_train, cat_features=cat_features)\n",
    "    valid_pool = Pool(x_valid, y_valid, cat_features=cat_features)\n",
    "    model.fit(train_pool, eval_set=valid_pool, early_stopping_rounds=100)\n",
    "    \n",
    "    # Make predictions\n",
    "    oof_pred = model.predict(x_valid)\n",
    "    test_pred = model.predict(x_test)\n",
    "    \n",
    "    # Store results\n",
    "    cat_oof[valid_idx] = oof_pred\n",
    "    cat_pred += test_pred / FOLDS\n",
    "    \n",
    "    # Calculate RMSLE\n",
    "    rmsle = np.sqrt(mean_squared_log_error(np.expm1(y_valid), np.expm1(oof_pred)))\n",
    "    cat_rmsle.append(rmsle)\n",
    "    \n",
    "    print(f\"Fold {i+1} RMSLE: {rmsle:.6f}\")\n",
    "    print(f\"Training time: {time.time() - start:.1f} sec\")\n",
    "\n",
    "# Calculate and display performance\n",
    "mean_rmsle = np.mean(cat_rmsle)\n",
    "std_rmsle = np.std(cat_rmsle)\n",
    "print(f\"\\nCatBoost - Mean RMSLE: {mean_rmsle:.6f} ± {std_rmsle:.6f}\")\n",
    "\n",
    "# Save results\n",
    "np.save('catboost_oof.npy', cat_oof)\n",
    "np.save('catboost_pred.npy', cat_pred)\n",
    "\n",
    "# Create a basic submission file with just CatBoost predictions\n",
    "cat_submission = submission.copy()\n",
    "cat_submission['Calories'] = np.clip(np.expm1(cat_pred), 1, 314)\n",
    "cat_submission.to_csv('catboost_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a775a83b-d250-4c08-a532-bcae96bb61de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training XGBoost model\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Improved XGBoost params\n",
    "xgb_params = {\n",
    "    'max_depth': 10,\n",
    "    'learning_rate': 0.01,\n",
    "    'min_child_weight': 3,\n",
    "    'gamma': 0.01,\n",
    "    'subsample': 0.9,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'colsample_bylevel': 0.8,\n",
    "    'reg_alpha': 0.05,\n",
    "    'reg_lambda': 0.1,\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'rmse',\n",
    "    'objective': 'reg:squarederror',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "FOLDS = 10\n",
    "bins = pd.qcut(y, 20, labels=False, duplicates='drop')\n",
    "kf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "xgb_oof = np.zeros(len(X))\n",
    "xgb_pred = np.zeros(len(X_test))\n",
    "xgb_rmsle = []\n",
    "\n",
    "print(\"\\n=== Training XGBoost ===\")\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(kf.split(X, bins)):\n",
    "    print(f\"\\nFold {i+1}\")\n",
    "    x_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    x_valid, y_valid = X.iloc[valid_idx], y.iloc[valid_idx]\n",
    "\n",
    "    x_train = x_train.loc[:, ~x_train.columns.duplicated()]\n",
    "    x_valid = x_valid.loc[:, ~x_valid.columns.duplicated()]\n",
    "    x_test = X_test.loc[:, ~X_test.columns.duplicated()].copy()\n",
    "\n",
    "    # Convert to DMatrix\n",
    "    dtrain = xgb.DMatrix(data=x_train, label=y_train)\n",
    "    dvalid = xgb.DMatrix(data=x_valid, label=y_valid)\n",
    "    dtest = xgb.DMatrix(data=x_test)\n",
    "\n",
    "    evals = [(dtrain, 'train'), (dvalid, 'eval')]\n",
    "\n",
    "    start = time.time()\n",
    "    model = xgb.train(\n",
    "        params=xgb_params,\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=3000,\n",
    "        evals=evals,\n",
    "        early_stopping_rounds=100,\n",
    "        verbose_eval=100\n",
    "    )\n",
    "\n",
    "    oof_pred = model.predict(dvalid)\n",
    "    test_pred = model.predict(dtest)\n",
    "\n",
    "    xgb_oof[valid_idx] = oof_pred\n",
    "    xgb_pred += test_pred / FOLDS\n",
    "\n",
    "    rmsle = np.sqrt(mean_squared_log_error(np.expm1(y_valid), np.expm1(oof_pred)))\n",
    "    xgb_rmsle.append(rmsle)\n",
    "\n",
    "    print(f\"Fold {i+1} RMSLE: {rmsle:.6f}\")\n",
    "    print(f\"Training time: {time.time() - start:.1f} sec\")\n",
    "\n",
    "# Final evaluation\n",
    "mean_rmsle = np.mean(xgb_rmsle)\n",
    "print(f\"\\nXGBoost - Mean RMSLE: {mean_rmsle:.6f}\")\n",
    "\n",
    "# Save results\n",
    "np.save('xgboost_oof.npy', xgb_oof)\n",
    "np.save('xgboost_pred.npy', xgb_pred)\n",
    "\n",
    "xgb_submission = submission.copy()\n",
    "xgb_submission['Calories'] = np.clip(np.expm1(xgb_pred), 1, 314)\n",
    "xgb_submission.to_csv('xgboost_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b04c9844-5768-41fb-979b-948f3347c411",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lightgbm import LGBMRegressor\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Improved LightGBM params\n",
    "lgb_params = {\n",
    "    'n_estimators': 3000,\n",
    "    'learning_rate': 0.01,\n",
    "    'max_depth': 12,\n",
    "    'num_leaves': 95,\n",
    "    'min_child_samples': 20,\n",
    "    'subsample': 0.85,\n",
    "    'colsample_bytree': 0.7,\n",
    "    'reg_alpha': 0.05,\n",
    "    'reg_lambda': 0.1,\n",
    "    'n_jobs': -1,\n",
    "    'verbose': -1,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "FOLDS = 10\n",
    "bins = pd.qcut(y, 20, labels=False, duplicates='drop')\n",
    "kf = StratifiedKFold(n_splits=FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "lgb_oof = np.zeros(len(X))\n",
    "lgb_pred = np.zeros(len(X_test))\n",
    "lgb_rmsle = []\n",
    "\n",
    "print(\"\\n=== Training LightGBM ===\")\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(kf.split(X, bins)):\n",
    "    print(f\"\\nFold {i+1}\")\n",
    "    x_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "    x_valid, y_valid = X.iloc[valid_idx], y.iloc[valid_idx]\n",
    "    \n",
    "    x_train = x_train.loc[:, ~x_train.columns.duplicated()]\n",
    "    x_valid = x_valid.loc[:, ~x_valid.columns.duplicated()]\n",
    "    x_test = X_test.loc[:, ~X_test.columns.duplicated()].copy()\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    # Train model\n",
    "    model = LGBMRegressor(**lgb_params)\n",
    "    model.fit(\n",
    "        x_train, y_train,\n",
    "        eval_set=[(x_valid, y_valid)],\n",
    "        callbacks=[early_stopping(stopping_rounds=100), log_evaluation(100)]\n",
    "    )\n",
    "    \n",
    "    # Make predictions\n",
    "    oof_pred = model.predict(x_valid)\n",
    "    test_pred = model.predict(x_test)\n",
    "    \n",
    "    # Store results\n",
    "    lgb_oof[valid_idx] = oof_pred\n",
    "    lgb_pred += test_pred / FOLDS\n",
    "    \n",
    "    # Calculate RMSLE\n",
    "    rmsle = np.sqrt(mean_squared_log_error(np.expm1(y_valid), np.expm1(oof_pred)))\n",
    "    lgb_rmsle.append(rmsle)\n",
    "    \n",
    "    print(f\"Fold {i+1} RMSLE: {rmsle:.6f}\")\n",
    "    print(f\"Training time: {time.time() - start:.1f} sec\")\n",
    "\n",
    "# Calculate and display performance\n",
    "mean_rmsle = np.mean(lgb_rmsle)\n",
    "std_rmsle = np.std(lgb_rmsle)\n",
    "print(f\"\\nLightGBM - Mean RMSLE: {mean_rmsle:.6f} ± {std_rmsle:.6f}\")\n",
    "\n",
    "# Save results\n",
    "np.save('lightgbm_oof.npy', lgb_oof)\n",
    "np.save('lightgbm_pred.npy', lgb_pred)\n",
    "\n",
    "# Create submission\n",
    "lgb_submission = submission.copy()\n",
    "lgb_submission['Calories'] = np.clip(np.expm1(lgb_pred), 1, 314)\n",
    "lgb_submission.to_csv('lightgbm_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2f83a79c-9ea0-44f5-ab96-778c2f485691",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Creating Advanced Ensemble ===\n",
      "Loaded CatBoost OOF predictions\n",
      "Loaded XGBoost OOF predictions\n",
      "Loaded LightGBM OOF predictions\n",
      "\n",
      "Error in ensemble creation: True target values file not found\n",
      "Creating simple average ensemble as fallback...\n",
      "Could not create fallback submission: name 'model_names' is not defined\n",
      "\n",
      "=== Ensemble Process Complete ===\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_log_error\n",
    "from scipy.optimize import minimize\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, HuberRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "print(\"\\n=== Creating Advanced Ensemble ===\")\n",
    "\n",
    "try:\n",
    "    # Load saved predictions\n",
    "    available_models = []\n",
    "    try:\n",
    "        cat_oof = np.load('catboost_oof.npy')\n",
    "        available_models.append(('catboost', cat_oof))\n",
    "        print(\"Loaded CatBoost OOF predictions\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"CatBoost OOF predictions not found\")\n",
    "    \n",
    "    try:\n",
    "        xgb_oof = np.load('xgboost_oof.npy')\n",
    "        available_models.append(('xgboost', xgb_oof))\n",
    "        print(\"Loaded XGBoost OOF predictions\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"XGBoost OOF predictions not found\")\n",
    "    \n",
    "    try:\n",
    "        lgb_oof = np.load('lightgbm_oof.npy')\n",
    "        available_models.append(('lightgbm', lgb_oof))\n",
    "        print(\"Loaded LightGBM OOF predictions\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"LightGBM OOF predictions not found\")\n",
    "    \n",
    "    # Check if we have at least 2 models to ensemble\n",
    "    if len(available_models) < 2:\n",
    "        raise ValueError(\"Need at least 2 models for ensembling\")\n",
    "    \n",
    "    # Load true target values\n",
    "    try:\n",
    "        y_true = np.load('true_target.npy')\n",
    "        print(\"Loaded true target values\")\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(\"True target values file not found\")\n",
    "    \n",
    "    # Prepare data for ensemble\n",
    "    model_names, oof_predictions = zip(*available_models)\n",
    "    X_stack = np.column_stack(oof_predictions)\n",
    "    \n",
    "    # Define ensemble optimization function\n",
    "    def objective(weights):\n",
    "        combined_pred = np.average(X_stack, axis=1, weights=weights)\n",
    "        return np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(combined_pred)))\n",
    "    \n",
    "    # Optimize weights\n",
    "    initial_weights = np.ones(len(available_models)) / len(available_models)\n",
    "    bounds = [(0, 1) for _ in range(len(available_models))]\n",
    "    constraints = ({'type': 'eq', 'fun': lambda w: np.sum(w) - 1})\n",
    "    \n",
    "    print(\"\\nOptimizing ensemble weights...\")\n",
    "    result = minimize(\n",
    "        objective,\n",
    "        initial_weights,\n",
    "        method='SLSQP',\n",
    "        bounds=bounds,\n",
    "        constraints=constraints,\n",
    "        options={'disp': True}\n",
    "    )\n",
    "    \n",
    "    optimal_weights = result.x\n",
    "    print(\"\\nOptimal weights:\")\n",
    "    for name, weight in zip(model_names, optimal_weights):\n",
    "        print(f\"{name}: {weight:.4f}\")\n",
    "    \n",
    "    # Create weighted ensemble predictions\n",
    "    ensemble_oof = np.average(X_stack, axis=1, weights=optimal_weights)\n",
    "    ensemble_rmsle = np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(ensemble_oof)))\n",
    "    print(f\"\\nEnsemble RMSLE: {ensemble_rmsle:.6f}\")\n",
    "    \n",
    "    # Load test predictions\n",
    "    test_predictions = []\n",
    "    for name in model_names:\n",
    "        try:\n",
    "            pred = np.load(f'{name}_pred.npy')\n",
    "            test_predictions.append(pred)\n",
    "            print(f\"Loaded {name} test predictions\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"{name} test predictions not found\")\n",
    "            test_predictions.append(None)\n",
    "    \n",
    "    # Create final ensemble prediction\n",
    "    valid_test_preds = [pred for pred in test_predictions if pred is not None]\n",
    "    if len(valid_test_preds) != len(optimal_weights):\n",
    "        print(\"Warning: Not all test predictions available, using equal weights\")\n",
    "        optimal_weights = np.ones(len(valid_test_preds)) / len(valid_test_preds)\n",
    "    \n",
    "    ensemble_test_pred = np.average(np.column_stack(valid_test_preds), axis=1, weights=optimal_weights)\n",
    "    \n",
    "    # Create stacked features for meta-model\n",
    "    print(\"\\nCreating stacked features for meta-model...\")\n",
    "    X_meta_train = X_stack\n",
    "    X_meta_test = np.column_stack(valid_test_preds)\n",
    "    \n",
    "    # Train meta-model (Stacking)\n",
    "    meta_models = [\n",
    "        ('ridge', Ridge(alpha=0.1, random_state=42)),\n",
    "        ('lasso', Lasso(alpha=0.0005, random_state=42)),\n",
    "        ('elasticnet', ElasticNet(alpha=0.0005, l1_ratio=0.7, random_state=42)),\n",
    "        ('huber', HuberRegressor(epsilon=1.35, alpha=0.0005))\n",
    "    ]\n",
    "    \n",
    "    stacking_regressor = StackingRegressor(\n",
    "        estimators=meta_models,\n",
    "        final_estimator=CatBoostRegressor(\n",
    "            iterations=1000,\n",
    "            learning_rate=0.01,\n",
    "            depth=6,\n",
    "            random_seed=42,\n",
    "            verbose=0\n",
    "        ),\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    print(\"Training stacking regressor...\")\n",
    "    stacking_regressor.fit(X_meta_train, y_true)\n",
    "    \n",
    "    # Get stacking predictions\n",
    "    stacking_oof = stacking_regressor.predict(X_meta_train)\n",
    "    stacking_test_pred = stacking_regressor.predict(X_meta_test)\n",
    "    \n",
    "    stacking_rmsle = np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(stacking_oof)))\n",
    "    print(f\"Stacking RMSLE: {stacking_rmsle:.6f}\")\n",
    "    \n",
    "    # Blend simple weighted average with stacking\n",
    "    print(\"\\nBlending weighted average and stacking...\")\n",
    "    blend_oof = 0.7 * ensemble_oof + 0.3 * stacking_oof\n",
    "    blend_test_pred = 0.7 * ensemble_test_pred + 0.3 * stacking_test_pred\n",
    "    \n",
    "    blend_rmsle = np.sqrt(mean_squared_log_error(np.expm1(y_true), np.expm1(blend_oof)))\n",
    "    print(f\"Blended RMSLE: {blend_rmsle:.6f}\")\n",
    "    \n",
    "    # Create final submission\n",
    "    final_submission = submission.copy()\n",
    "    final_submission['Calories'] = np.clip(np.expm1(blend_test_pred), 1, 314)\n",
    "    final_submission.to_csv('final_ensemble_submission.csv', index=False)\n",
    "    print(\"\\nSaved final ensemble submission!\")\n",
    "    \n",
    "    # Plot feature importance for stacking\n",
    "    try:\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        # Get feature importance from the meta-model\n",
    "        if hasattr(stacking_regressor.final_estimator_, 'feature_importances_'):\n",
    "            importance = stacking_regressor.final_estimator_.feature_importances_\n",
    "        elif hasattr(stacking_regressor.final_estimator_, 'coef_'):\n",
    "            importance = np.abs(stacking_regressor.final_estimator_.coef_)\n",
    "        else:\n",
    "            importance = None\n",
    "            \n",
    "        if importance is not None:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.bar(range(len(importance)), importance)\n",
    "            plt.xticks(range(len(model_names)), model_names, rotation=45)\n",
    "            plt.title('Stacking Meta-Model Feature Importance')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('stacking_importance.png')\n",
    "            plt.close()\n",
    "            print(\"Saved stacking importance plot\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not create importance plot: {str(e)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nError in ensemble creation: {str(e)}\")\n",
    "    print(\"Creating simple average ensemble as fallback...\")\n",
    "    \n",
    "    try:\n",
    "        # Simple average fallback\n",
    "        test_predictions = []\n",
    "        for name in model_names:\n",
    "            try:\n",
    "                pred = np.load(f'{name}_pred.npy')\n",
    "                test_predictions.append(pred)\n",
    "            except:\n",
    "                continue\n",
    "        \n",
    "        if len(test_predictions) > 0:\n",
    "            avg_test_pred = np.mean(np.column_stack(test_predictions), axis=1)\n",
    "            final_submission = submission.copy()\n",
    "            final_submission['Calories'] = np.clip(np.expm1(avg_test_pred), 1, 314)\n",
    "            final_submission.to_csv('simple_average_submission.csv', index=False)\n",
    "            print(\"Saved simple average submission\")\n",
    "    except Exception as e2:\n",
    "        print(f\"Could not create fallback submission: {str(e2)}\")\n",
    "\n",
    "print(\"\\n=== Ensemble Process Complete ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39a5462-5b67-4898-a140-a6839bc65e21",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
